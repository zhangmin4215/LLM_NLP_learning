#### 1.引言
LLM(大语言模型) 和 PLM(预训练语言模型) 

#### 2.概述
#### 2.1 大语言模型的涌现能力
LLM的涌现能力被正式定义为 “在小型模型中不存在但在大型模型中产生的能力”

（1）上下文学习(In-Context Learning, ICL)

（2）指令遵循

（3）逐步推理：通过使用思维链（Chain-of-Thought, CoT）提示策略，LLM 可以通 过利用包含中间推理步骤的提示机制来解决这类任务，从而得出最终答案。

#### 2.2 大语言模型的关键技术
1.扩展：Transfomer语言模型存在明显的扩展效应，更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升。作为两个代表性的模 型，GPT-3 和 PaLM 通过增加模型规模分别达到了 1750 亿 和 5400 亿。

2.训练：分布式训练算法是学习 LLM 网络参数所必需的

3.能力引导：在大规模语料库上预训练之后，LLM 具备 了作为通用任务求解器的潜在能力。设计合适的任务指令或具体的 ICL 策略可以激发这些能力。

4.对齐微调：InstructGPT 设计了一种有效的微调方法，使 LLM 能够 按照期望的指令进行操作，其中利用了基于人类反馈的强化学习技术。

5.工具操作：利用外部工具可以进一步扩展 LLM 的能力。例如，LLM 可以利用计算器进行准确计算， 利用搜索引擎检索未知信息。

#### 3.2 常用语料库
将这些语料库分为六个组别进行介绍：Books、 CommonCrawl、Reddit Links、Wikipedia、Code、Others

#### 4.预训练
#### 4.1 数据收集
语料库的来源可以广义地分为两种类型：通用文本数据和专用文本数据。

* 通用文本数据：网页爬取、对话文本、书籍

* 专用文本数据：arXiv 论文、科学教材、数学网页和其他相关的科学资源。

#### 4.2 数据预处理
（1）质量过滤：基于分类器的方法，和基于启发式的方法。前一种方法基于高质量文本训练“选择分类器”，并利用它来识别和过滤低质量数据。第二种方法，采用基于启发式的方法，通过设计一组精心设计的规则来消除低质量文本，如基于度量(困惑度)、统计、关键词的过滤。

（2）去重：现有的研究发现，语料库中的重复数据会降低语言模型的多样性，可能导致训练过程不稳定，从而影响模型性能。可以在句子级、文档级和数据集级等不同粒度上去重。

（3）隐私过滤：大多数预训练文本数据来自网络来源，包括涉及敏感或个人信息的用户生成内容，这可能增加隐私泄露的风险。采用基于规则的方法，例如关键字识别，姓名、地址和电话号码。

（4）分词：它的目的是将原始文本分割成词序列，随后用作 LLM 的输入。使用专门为预训练语料库设计的分词器可能会更加有效







